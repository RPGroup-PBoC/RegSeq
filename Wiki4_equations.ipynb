{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Math Behind the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Chain Monte Carlo\n",
    "\n",
    "Our preferred method for determining the positions of putative TF binding sites, as well as to understand how mutations impact gene expression, is to perform Markov Chain Monte Carlo (MCMC) inference. MCMC gets its name from two processes, Monte Carlo and Markov Chain. Monte Carlo is a method for estimating features of a distribution by randomly drawing samples from the distribution. For example, one could estimate the mean or standard deviation of a distribution by drawing random samples and computing the mean and standard deviation for those samples. MCMC methods are often used when functions are not amenable to analytical solutions or calculations. MCMC methods allow the expectation value of a given parameter, and its uncertainty without requiring us to have full access to the underlying probability distribution. As with many cases in biology, the true underlying probability distribution is often complicated and difficult to access.\n",
    "\n",
    "Our Reg-Seq method currently uses human intuition to determine binding sites (based on the characteristics of information footprints, discussed later in this Wiki protocol), but to validate our binding site choices, and to capture all regions of a sequence that are important for gene expression, we need to also computationally identify regions where gene expression is changed significantly up or down by mutation (p < 0.01), and discard any potential sites which do not fit this criteria. We infer the effect of mutation using MCMC and we use the distribution of parameters from the inference to form a 99 % confidence interval for the average effect of mutation across a 15 base pair region. We include binding sites that are statistically significant at the 0.01 level in any of the tested growth conditions.\n",
    "\n",
    "One difficulty with estimating the mutual information from model predictions is that base pair identity A, C, G, T and the gene expression level µ are both discrete variables, while binding energy predictions from the model (x) is\n",
    "a continuous variable. Formally, the mutual information is given by\n",
    "\n",
    "\\begin{align}\n",
    "I(\\mu, x) = \\int_{- \\infty}^{+ \\infty} dx \\sum_{u}^{} p(x, \\mu) \\log_{2} \\left( \\frac{p(x, \\mu)}{p(x) p(\\mu)} \\right) \\tag{1}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "where µ is a measure of gene expression,\n",
    "\n",
    "\\begin{align}\n",
    "\\mu = \\begin{cases}\n",
    "  0, \\quad \\text{for sequencing reads from DNA library}\\\\\n",
    "  1, \\quad \\text{for sequencing reads originating from mRNA} \\tag{2}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "during Reg-Seq.\n",
    "\n",
    "The probability distribution, $p$, is not one that we have full access to, as we only have a discrete set of predictions (one from each of the N unique DNA sequences in our data set). To compensate for the issues that can arise from estimating a continuous distribution from discrete data, we make use of the fact that any transformation that preserves the rank order (for instance multiplying all model predictions by a constant) the mutual information is unchanged.\n",
    "\n",
    "We will define $z_q$ as the rank order in binding energy predictions of the $qth$ sequence. We estimate $I(\\mu,z)$ by first calculating binding energy predictions\n",
    "\n",
    "\\begin{align}\n",
    "x = \\sum_{i=1}^L \\sum_{j=A}^T \\theta_{ij} \\cdot s_{ij} \\tag{3}\n",
    "\\end{align}\n",
    "\n",
    "and then converting them to a rank order predictions $z$.\n",
    "\n",
    "We then discretize the energy predictions into 1000 \"bins\" and convolve with a Gaussian kernel to estimate the probability distribution $p(z, \\mu)$. We can then calculate the mutual information with\n",
    "\n",
    "\\begin{align}\n",
    "I(\\mu,z)_{smoothed} = \\sum_{z=1}^{1000} \\sum{\\mu} F(\\mu,z) \\log_2 \\frac{F(\\mu,z)}{F(z) \\cdot F(\\mu)} \\tag{4}\n",
    "\\end{align}\n",
    "\n",
    "where $F(\\mu,z)$ is the probability distribution and $p(\\mu,z)$ is estimated from finite data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Chain Monte Carlo fitting procedure\n",
    "\n",
    "As proven by [Justin Kinney (2008)](https://pdfs.semanticscholar.org/56ce/a3cb3609844a0df0554f99524dbb96479c2d.pdf) in his disseration, \"Biophysical Models of Transcriptional Regulation from Sequence Data\", the likelihood of a model that predicts gene output is\n",
    "\n",
    "\\begin{align}\n",
    "L(\\theta|{\\mu_s}) \\propto 2^{NI_{smooth}(\\mu,z)} \\tag{5}\n",
    "\\end{align}\n",
    "\n",
    "where $N$ is the total number of independent sequences, $I_{smooth}$ is the smoothed mutual information between gene expression, $\\mu$, and DNA sequence $z$.\n",
    "\n",
    "The probability distributions are very difficult to handle analytically. The reason why we use MCMC is that you can estimate properties using the target probability distribution without needing to know the distribution. For example, we can estimate $\\left\\langle \\theta \\right \\rangle$ by drawing many samples of $\\theta$ using MCMC and taking the mean of the parameters.\n",
    "\n",
    "We therefore need to construct a Markov Chain whose stationary distribution converges to the distribution of interest $p(\\theta)$. A Markov chain is a sequence of values ${\\theta_1, \\theta_2,\\theta_3,..., \\theta_N}$ for $N$ steps. We can then find $\\left \\langle \\theta \\right \\rangle$ with\n",
    "\n",
    "\\begin{align}\n",
    "\\left \\langle \\theta \\right \\rangle = \\frac{\\sum_{N=1}^{100} \\theta_N}{N} \\tag{6}\n",
    "\\end{align}\n",
    "\n",
    "A Markov chain has no memory. That is the probability that the $N^{th}$ value in the chain takes a value $\\theta_N$ depends only on the $N-1^{th}$ value in the chain. To make things a bit more concrete, let's leave aside $\\theta$ for the time being. Imagine that we have a light switch, and we know the switch is \"on\" 25% of the time and \"off\" 75% of the time. For each \"step\" in our Markov chain, we can change the state of the switch; if the switch is on, we turn it off with some rate $k_{off}$, and if the switch is currently off, we turn it off with the rate $k_{on}$. A sequence of states will be generated. One example would be [off, off, off, on, on, on, off]. These states constitute a Markov chain and if the chain is continued for long enough, the stationary distribution will converge such that $p_{on}$ = 0.25 and $p_{off}$ = 0.75.\n",
    "\n",
    "A Markov chain is stationary if detailed balance is satisfied between its states. The condition of detailed balance obtains if the total rate of transitions from on to off is the same as the total rate of transitions from off to on. Mathematically, this condition can be written as\n",
    "\n",
    "\\begin{align}\n",
    "k_{on} \\times p_{off} = k_{off} \\times p_{on} \\tag{7}\n",
    "\\end{align}\n",
    "\n",
    "This equation allows us to calculate $k_{on}$ and $k_{off}$. It follows that \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{k_{on}}{k_{off}} = \\frac{p_{on}}{p_{off}}  = \\frac{1}{3} \\tag{8}\n",
    "\\end{align}\n",
    "\n",
    "As long as the ratio of transition rates is satisfied and enough steps are taken, then the stationary distribution will converge to the proper distribution.\n",
    "\n",
    "For the far more complicated task of estimating $p(\\theta)$, we can fall back on the standard Metroplis-Hastings sampling algorithm. $\\theta$ will be a matrix where $\\theta_{ij}$ will be the energetic contributions to the energy matrix for the $i^{th}$ position and the $j^{th}$ base pair where $i \\in {1,2,3,...,L}$ and $j \\in {A,C,G,T}$. We can then follow the procedure:\n",
    "\n",
    "1. Start with a random energy matrix $\\theta_0$.\n",
    "2. Make a random perturbation $d \\theta_0$ to $\\theta_0$. This perturbation will have a small adjustment to each element of $\\theta$.\n",
    "3. Compute the model likelihoods $L(\\theta_0)$ and $L(\\theta_0 + d\\theta$ using (Eqn. 5).\n",
    "4. If $L(\\theta_0)$ and $L(\\theta_0 + d \\theta > L(\\theta$, accept the new parameter values $\\theta_0$ + $d\\theta$ as the next element in the Markov chain $\\theta_1$. Otherwise, accept $\\theta_0$ + $d \\theta$ with probability $\\frac{L(\\theta_0 + d\\theta)}{L(\\theta_0)}$. If the step is rejected, the next element $\\theta_1$ in the Markov chain is reset to its previous value $\\theta_0$. The acceptance/rejection probabilities mean that detailed balance is satisfied between the states $\\theta_0$ and $\\theta_0 + d\\theta$.\n",
    "5. Repeat steps 2-4 of this procedure until the chain converges to the stationary distribution. In practice this can be determined by monitoring when the mutual information plateaus.\n",
    "6. To be certain that the distribution has in fact converged to the proper stationary distribution, multiple Markov Chains should be run starting with step 1. If all the chains converge to the same distribution, then they have properly converged.\n",
    "\n",
    "The end result of these model-fitting efforts is an optimized linear binding energy matrix. You can get a measure of the uncertainty in $\\theta_{ij}$ by forming a confidence interval out of the distribution of parameters formed from the Markov Chain. This inference is performed using the MPAthic software.\n",
    "\n",
    "Now that we have discussed MCMC, let's now turn to the other _potential_ method for data analysis: least squares analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares Analysis\n",
    "\n",
    "In the Reg-Seq paper, we used exclusively MCMC analysis to interpret data emanating from sequencing experiments. However, it is still conceptually important to understand how Least Squares analysis works, as this offers an alternative method for analyzing the sequencing data.\n",
    "\n",
    "Our least squares inference can be found in a paper by [Ireland and Kinney, _bioRxiv_](https://www.biorxiv.org/content/10.1101/054676v2).\n",
    "\n",
    "Least squares provides a computationally simple inference procedure that overcomes the most onerous restrictions of enrichment ratio calculations. It can be used to infer any type of linear model, including both matrix models and neighbor models. It can also be used on data that consists of more than two bins. The idea behind the least squares approach is to\n",
    "choose parameters $\\theta^{LS}$ that minimize a quadratic loss function. Specifically, we use\n",
    "\n",
    "\\begin{align}\n",
    "\\theta^{LS} = argmin_{\\theta} L(\\theta) \\tag{9}\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{align}\n",
    "L(\\theta) = \\sum_M \\sum_{n | M} \\frac{[r (S^n, \\theta) - \\mu_M]^2 }{\\sigma_M^2} + \\alpha \\sum_i \\theta_i^2 \\tag{10}\n",
    "\\end{align}\n",
    "\n",
    "Here, $\\mu_M$ is the assumed mean activity of sequences in bin $M$, $\\sigma_M^2$ is the assumed variance in the activities of such sequences, $i$ indexes all parameters in the model, and $\\alpha$ is the \"ridge regression\" regularization parameter. By using the objective function $L(\\theta)$, one can rapidly compute values of the optimal parameters $\\theta$ using standard algorithms.\n",
    "\n",
    "One downside to least squares inference is the need to assume specific values for $\\mu_M$ and for $\\sigma_M^2$ for each bin $M$. MPAthic allows the user to manually specify these values. There is a danger here, since assuming incorrect values for $\\mu_M$ and $\\sigma_M^2$ will generally lead to bias in the inferred parameters $\\theta^{LS}$. In practice, however, the default choice of $\\mu_M = M$ and $\\sigma_M^2 = 1$ often works surprisingly well when bins are arranged from lowest to highest average activity. Another downside to least squares is the need to assume that experimental noise – specifically, $p(R|M)$ - is Gaussian. Only in such cases does least squares inference correspond to a meaningful maximum likelihood calculation. In massively parallel assays, however, noise is often strongly non-Gaussian. In such situations, least squares inference cannot be expected to yield correct model parameters for any choice of $\\mu_M = M$ and $\\sigma_M^2 = 1$.\n",
    "\n",
    "Now that we have discussed both MCMC and least squares analysis, we now turn our attention to the _visualization_ of our analyzed data. Now that we have coaxed the sequencing data into a usable format -- and performed inference on the data -- we get to the fun part: plotting the data! In the [original Reg-Seq](https://www.biorxiv.org/content/10.1101/2020.01.18.910323v3) paper, we visualize sequencing data in three ways: information footprints, sequence logos and energy matrices. We discuss the _mathematics_ behind each of these visual formats next, and then conclude this Wiki protocol with in-depth details on how we plot these three visual tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Math Behind the Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematics of Information Footprints\n",
    "\n",
    "To determine putative transcription factor binding sites, we first compute the effect of mutations on gene expression at a base pair-by-base pair level using information footprints. The information footprints are a hypothesis generating tool and we choose which regions to further investigate using techniques such as mass spectrometry by visually inspecting the data for regions of 10 to 20 base pairs that have high information content compared to background.\n",
    "\n",
    "We use information footprints as a tool for hypothesis generation to identify regions which may contain transcription factor binding sites. In general, a mutation within a transcription factor site is likely to severely weaken that site. We look for groups of positions where a mutation away from the wildtype has a large effect on gene expression. A simplified data set on a four nucleotide sequence then might\n",
    "look like this table.\n",
    "\n",
    "| Sequence    | DNA Counts  | mRNA Counts |\n",
    "| ----------- | ----------- | ----------- |\n",
    "| ACTA        | 5           | 23          |\n",
    "| ATTA        | 5           | 3           |\n",
    "| CCTG        | 11          | 11          |\n",
    "| TAGA        | 12          | 3           |\n",
    "| GTGC        | 2           | 0           |\n",
    "| CACA        | 8           | 7           |\n",
    "| AGGC        | 7           | 3           |\n",
    "| ...         | ...         | ...         |\n",
    "\n",
    "_Table: A sample data set that consists of numerous, four-nucleotide sequences, a column for DNA counts and a column for cDNA counts. Here, the value in the DNA column for each sequence is determined via sequencing of the DNA libraries, and counting how many times the given, 4-nucleotide sequence appeared. A similar process, albeit with cDNA, is performed to determine how many times each sequence appears in RNA form. Dividing RNA / DNA provides a value of relative gene expression for a given DNA sequence._\n",
    "\n",
    "One possible calculation to measure the impact of a given mutation on expression is to take all sequences which have base $b$ at position $i$ and determine the number of mRNAs produced per read in the sequencing library. By comparing the values for different bases we could determine how large of an effect mutation has on gene expression. We plot information in bits, the output of the formula for Shannon information entropy. The information entropy of a coin flip is one bit. (so knowing the outcome would be 1 bit of information).\n",
    "\n",
    "In the table above, the frequency of the different nucleotides in the library at position 2 is 40% A, 32% C, 14% G and 14% T. Cytosine is enriched in the mRNA transcripts over the original library, as it now composes 68% of all mRNA sequencing reads while A, G, and T only compose only 20%, 6%, and 6% respectively. Large enrichment of some bases over others occurs when base identity is important for gene expression. We can quantify how important using the mutual information between base identity and gene expression level. Mutual information is given at position $i$ by\n",
    "\n",
    "\\begin{align}\n",
    "I_b =  \\sum_{m=0}^1  \\sum_{\\mu=0}^1 p(m,\\mu)\\log_2\\left(\\frac{p(m,\\mu)}{p_{mut}(m)p_{expr}(\\mu)}\\right) \\tag{11}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "$p_{mut}(m)$ in this equation refers to the probability that a given sequencing read will be from a mutated base, and $p_{expr}(\\mu)$ is a normalizing factor that gives the ratio of the number of DNA or mRNA sequencing counts to total number of counts.\n",
    "\n",
    "The mutual information quantifies how much a piece of knowledge reduces the entropy of a distribution. At a position where base identity matters little for expression level, there would be little difference in the frequency distributions for the library and mRNA transcripts. The entropy of the distribution would decrease only by a small amount when considering the two types of sequencing reads separately.\n",
    "\n",
    "We are interested in quantifying the degree to which mutation away from a wild type sequence affects expression. Although there are obviously four possible nucleotides, we can classify each base as either wild-type or mutated so that ![b](https://render.githubusercontent.com/render/math?math=b) in **(Eqn. 11)** represents only these two possibilities.\n",
    "\n",
    "If mutations at each position are not fully independent, then the information value calculated in **(Eqn. 11)** will also encode the effect of mutation at correlated positions. If having a mutation at position 1 is highly favorable for gene expression and is also correlated with having a mutation at position 2, mutations at position 2 will also be enriched amongst the mRNA transcripts. Position 2 will appear to have high mutual information even if it has minimal effect on gene expression.\n",
    "\n",
    "Due to the DNA synthesis process used in library construction, mutation in one position can make mutations at other positions more likely by up to 10 percent. This is enough to cloud the signature of most transcription factors in an information footprint calculated using **(Eqn. 11)**.\n",
    "\n",
    "We need to determine values for $p_{i}(m | \\mu)$ when mutations are independent, and to do this we need to fit these quantities from our data. We assert that\n",
    "\n",
    "\\begin{align}\n",
    "\\left\\langle mRNA \\right\\rangle \\propto e^{-\\beta E_{eff}} \\tag{12}\n",
    "\\end{align}\n",
    "\n",
    "is a reasonable approximation to make.\n",
    "\n",
    "In this equation, $\\left\\langle mRNA \\right\\rangle$ is the average number of mRNAs produced by that sequence for every cell containing the construct and $E_{eff}$ is an effective energy for the sequence that can be determined by summing contributions from each position in the sequence.\n",
    "\n",
    "There are many possible underlying regulatory architectures, but to demonstrate that our approach is reasonable let us first consider the simple case where there is only an RNAP binding site in the studied region. We can write down an expression for average gene expression per cell as\n",
    "\n",
    "\\begin{align}\n",
    "\\left\\langle mRNA \\right\\rangle \\propto p_{bound} \\propto \\frac{\\frac{p}{N_{NS}}e^{-\\beta E_P}}{1 + \\frac{p}{N_{NS}}e^{- \\beta E_P}} \\tag{13}\n",
    "\\end{align}\n",
    "\n",
    "Where $p_{bound}$ is the probability that the RNAP is bound to DNA and is known to be proportional to gene expression in _E. coli_, $E_p$ is the energy of RNAP binding, $N_{NS}$ is the number of nonspecific DNA binding sites, and $p$ is the number of RNAPs. If RNAP binds weakly, then $\\frac{p}{N_{NS}}e^{-\\beta E_p} << 1$. We can then simplify **(Eqn. 13)** to\n",
    "\n",
    "\\begin{align}\n",
    "\\left\\langle mRNA \\right\\rangle \\propto e^{- \\beta E_p} \\tag{14}\n",
    "\\end{align}\n",
    "\n",
    "If we assume that the energy of RNAP binding will be a sum of contributions from each of the positions within its binding site then we can calculate the difference in gene expression between having a mutated base at position $i$ and having a wild type base as\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\left\\langle mRNA_{WT_i} \\right\\rangle}{\\left\\langle mRNA_{Mut_i} \\right\\rangle} = \\frac{e^{- \\beta E_{P_{WT_i}}}}{e^{- \\beta E_{P_{Mut_i}}}} \\tag{15}\n",
    "\\end{align}\n",
    "\n",
    "Furthermore, we can conclude that\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\left\\langle mRNA_{WT_i} \\right\\rangle}{\\left\\langle mRNA_{Mut_i} \\right\\rangle} = e^{- \\beta \\left( E_{P_{WT_i}} - E_{P_{Mut_i}} \\right)} \\tag{16}\n",
    "\\end{align}\n",
    "\n",
    "In this example we are only considering a single mutation in the sequence, so we can further simplify **(Eqn. 16)** to\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\left\\langle mRNA_{WT_i} \\right\\rangle}{\\left\\langle mRNA_{Mut_i} \\right\\rangle} = e^{- \\beta \\Delta E_{P_i}} \\tag{17}\n",
    "\\end{align}\n",
    "\n",
    "We can now calculate the base probabilities in the expressed sequences. If the probability of finding a wild type base at position $i$ in the DNA library is $p_i(m=WT|\\mu=0)$, then $p_i(m=WT|\\mu=1)$ and this can be written as\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{p_i(m=WT|exp=0) \\frac{\\left\\langle mRNA_{WT_i} \\right\\rangle}{\\left\\langle mRNA_{Mut_i} \\right\\rangle}}{p_i(m=Mut|exp=0)  + p_i(m=WT|exp=0) \\frac{\\left\\langle mRNA_{WT_i} \\right\\rangle}{\\left\\langle mRNA_{Mut} \\right\\rangle}} \\tag{18}\n",
    "\\end{align}\n",
    "\n",
    "and $p_i(m=WT|\\mu=1)$ enables us to write an additional equation,\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{p_i(m=WT|exp=0) e^{- \\beta \\Delta E_{P_i}}}{p_i(m=Mut|exp=0)  + p_i(m=WT|exp=0) e^{- \\beta \\Delta E_{P_i}}} \\tag{19}\n",
    "\\end{align}\n",
    "\n",
    "Under certain conditions, we can also infer a value for $p_i (m | \\mu = 1)$ using a linear model when there are any number of activator or repressor binding sites. We will demonstrate this in the case of a single activator and a single repressor, although a similar analysis can be done when there are greater numbers of transcription factors.\n",
    "\n",
    "We will define $P = \\frac{p}{N_{NS}}e^{- \\beta E_P}$. We will also define $A = \\frac{a}{N_{NS}}e^{-\\beta E_A}$ where $a$ is the number of activators, and $E_A$ is the binding energy of the activity. Finally, we define $R = \\frac{r}{N_{NS}}e^{-\\beta E_R}$ where $r$ is the number of repressors and $E_R$ is the binding energy of the repressor. We can then write\n",
    "\n",
    "\\begin{align}\n",
    "\\left\\langle mRNA \\right\\rangle \\propto p_{bound} \\propto \\frac{P + PAe^{-\\beta \\epsilon_{AP}}}{1+A+P+R+PAe^{-\\beta \\epsilon_{AP}}} \\tag{20}\n",
    "\\end{align}\n",
    "\n",
    "If activators and RNAP bind weakly but interact strongly, and repressors bind very strongly, then we can simplify **(Eqn. 20)**. In this case $A$  << 1, $P$ << 1, $P A e^{-\\epsilon_{AP}}$ >> $P$, and $R$ >> 1. We can then rewrite **(Eqn. 20)** as\n",
    "\n",
    "\\begin{align}\n",
    "\\left\\langle mRNA \\right\\rangle \\propto \\frac{PAe^{-\\beta \\epsilon_{AP}}}{R} \\tag{21}\n",
    "\\end{align}\n",
    "\n",
    "which leads to\n",
    "\n",
    "\\begin{align}\n",
    "\\left\\langle mRNA \\right\\rangle \\propto e^{-\\beta(-E_P - E_A + E_R)} \\tag{22}\n",
    "\\end{align}\n",
    "\n",
    "As we typically assume that RNAP binding energy, activator binding energy, and repressor binding can all be represented as sums of contributions from their constituent bases (the nucleotides to which they bind), the combination of the energies can be written as a total effective energy $E_{eff}$ which is a sum of contributions from all positions within the binding sites.\n",
    "\n",
    "We fit the parameters for each base using MCMC. We always perform at least two MCMC runs, using the same conditions, and ensure that both chains reach the same distribution to prove the convergence of the chains. We do not wish for mutation rate to affect the information values, so we set the $p(\\text{WT}) = p(\\text{Mut}) = 0.5$ in the information calculation. The information values are smoothed by averaging with neighboring values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematics of Sequence Logos\n",
    "\n",
    "Sequence logos provide a simple way to visualize the sequence specificity of a transcription factor to DNA, as well as the amount of information present at each position. Here we describe how we generate them using either known genomic binding sites or the energy matrices from our Reg-Seq data. In each case we need to calculate a $4 \\times L$ position weight matrix for a binding site of length $L$, which is used to estimate the position-dependent information content that will then be used to construct a sequence logo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Position Weight Matrices from Known Genomic Binding Sites.\n",
    "\n",
    "To construct a position weight matrix using genomic binding sites, we must first align all the available binding site sequences and determine the nucleotide statistics at each position. Specifically, we count the number of each nucleotide, $N_{ij}$, at each position along the binding site. Here the subscript $i$ refers to the position, while $j$ refers to the nucleotide, A, C, G, or T. We can then calculate a position probability matrix (also $4 \\times L$ where each entry is found by dividing these counts by the total number of sequences in our alignment,\n",
    "\n",
    "\\begin{align}\n",
    "p_{ij} = \\frac{N_{ij}}{N_g} \\tag{23}\n",
    "\\end{align}\n",
    "\n",
    "Note that in situations where the number of aligned sequences is small (e.g., less than five), we typically add 1 pseudocount sequence to regularize the probabilities of the counts in the calculation of position probabilities,\n",
    "\n",
    "\\begin{align}\n",
    "p_{ij} = \\frac{N_{ij} + B_p}{N_g + 4 \\cdot B_p} \\tag{24}\n",
    "\\end{align}\n",
    "\n",
    "where $B_p$ is the value of the pseudocount. The argument for their use is that when selecting from a small number of binding site sequences, there is a chance that infrequent nucleotides will be absent, and assigning them a probability $p_{ij}$ of zero may be too stringent of a penalty. We let $B_p$ = 0.1. In the limit of zero binding site sequences (i..e with no sequences observed), this will result in probabilities $p_{ij}$ approximately equal to the background probability used in calculating the position weight matrix below (and a non-informative sequence logo).\n",
    "\n",
    "Finally, the values of the position weight matrix are found by calculating the log probabilities relative to a background model,\n",
    "\n",
    "\\begin{align}\n",
    "PW M_{ij} = log_2 \\frac{p_{ij}}{b_j} \\tag{25}\n",
    "\\end{align}\n",
    "\n",
    "The background model reflects assumptions about the genomic background of the system under investigation. For instance, in many cases it may be reasonable to assume each base is equally likely to occur. Given that we know the base frequencies for _E. coli_, we choose a background model that reflects these frequencies $b_j$: A = 0.246, C = 0.254, G = 0.254, and T = 0.246 for strain MG1655; [BioNumbers\n",
    "ID 100528](http://bionumbers.hms.harvard.edu). The value at the $i, j th$ position will be zero if the probability, $p_{ij}$, matches that of the background model, but non-zero otherwise. This reflects the fact that base frequencies matching the background model tell us nothing about the binding preferences of the transcription factor, while deviation from this background frequency indicates sequence specificity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Position Weight Matrices from Reg-Seq Data.\n",
    "\n",
    "Next we construct a position weight matrix using our Reg-Seq data. Here, we appeal to the result from [Berg and von Hippel](https://www.ncbi.nlm.nih.gov/pubmed/3612791), that the logarithms of the base frequencies above should be proportional to their binding energy contributions. Berg and von Hippel considered a statistical mechanical system containing $L$ independent binding site positions, with the choice of nucleotide at each position corresponding to a change in the energy level by $\\epsilon_{ij}$ relative to the lowest energy state at that position.\n",
    "\n",
    "$\\epsilon_{ij}$ corresponds to the energy entry from our energy matrix, scaled to absolute units, $\\epsilon_{ij}$ = $A \\dot \\theta_{ij} \\dot B$, where $\\theta_{ij}$ is the $i, jth$ entry. An important assumption is that all nucleotide sequences that provide an equivalent binding energy will have equal probability of being present as a binding site. In this way, we can relate the binding energies considered here to the statistical distribution of binding sites in the previous section. The probability $p_{ij}$ of choosing a nucleotide at position $i$ will then be proportional to the probability that position $i$ has energy $\\epsilon_{ij}$. Specifically, the probabilities will be given by their Boltzmann factors normalized by the sum of states for all nucleotides,\n",
    "\n",
    "\\begin{align}\n",
    "p_{ij} = \\frac{b_j \\cdot e^{-\\beta A \\cdot \\theta_{ij} \\cdot s_{ij}}}{\\sum_{j=A}^{T} b_j \\cdot e^{-\\beta A \\cdot \\theta_{ij} \\cdot s_{ij}}} \\tag{26}\n",
    "\\end{align}\n",
    "\n",
    "where $\\beta = 1 / k_B T$, $k_B$ is Boltzmann’s constant and $T$ is the absolute temperature. As above, $b_j$ refers to the background probabilities of each nucleotide. Note that the energy scaling factor B drops out of this equation since it is shared across each term. One difficulty that arises when we use energy matrices that are not in absolute energy units is that we are left with an unknown scale factor $A$, preventing calculation of $p_{ij}$. We appeal to the expectation that mismatches usually involve an energy cost of 1-3 $k_B T$. In other work within our group, we have\n",
    "found this to be a reasonable assumption for _LacI_. Therefore, we approximate it such that the average\n",
    "cost of a mutation is 2 $k_B T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Details on Sequence Logos\n",
    "\n",
    "With our position weight matrices in hand, we now construct sequence logos by calculating the average\n",
    "information content at each position along the binding site. With our four letter alphabet there is a\n",
    "maximum amount of information of 2 bits $[log_2 4$ = 2 bits) at each position $i$. The information content\n",
    "will be zero at a position when the nucleotide frequencies match the genomic background, and will have\n",
    "a maximum of 2 bits only if a specific nucleotide is completely conserved. The total information content\n",
    "at position $i$ is determined through calculation of the Shannon entropy, and is given by\n",
    "\n",
    "\\begin{align}\n",
    "I_i = \\sum_{j=A}^{T} p_{ij} \\cdot log_2 \\frac{p_{ij}}{b_i} = \\sum_{j=A}^{T} p_{ij} \\cdot PWM_{ij} \\tag{27}\n",
    "\\end{align}\n",
    "\n",
    "Here, $PWM_{ij}$ refers to the $i, jth$ entry in the position weight matrix. The total information content contained in the position weight matrix is then the sum of information content across the length of the binding site.\n",
    "\n",
    "To construct a sequence logo, the height of each letter at each position $i$ is determined by $SeqLogo_{ij} = p{ij} \\cdot I_i$, which is in units of bits. This causes each nucleotide in the sequence logo to be displayed as the proportion of the nucleotide expected at that position scaled by the amount of information contained at that position. To construct and plot sequence logos, we use custom Python code written by Justin Kinney; this code is discussed in a subsequent section of this protocol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematics of Energy Matrices\n",
    "\n",
    "Energy matrices can be inferred using Bayesian parameter estimation with an error-model-averaged likelihood, as\n",
    "previously described by [Kinney & Atwal](https://www.pnas.org/content/111/9/3354).\n",
    "\n",
    "Focusing on an individual putative transcription factor binding site, as revealed in an information footprint (which we discuss in the next section), we are next interested in developing a more fine-grained, quantitative understanding of how the underlying protein-DNA interaction is determined based on gene expression data. An energy matrix displays this information using a heat map format, where each column is a position in the putative binding site and each row displays the effect on binding that results from mutating to that given nucleotide (given as a change in the DNA-TF interaction energy upon mutation). These energy matrices are scaled such that the wild type sequence is colored in white, mutations that improve binding are shown in blue, and mutations that weaken binding are shown in red. These energy matrices encode a full quantitative picture for how we expect sequence to relate to binding for a given TF, such that we can provide a prediction for the binding energy of every possible binding site sequence as\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^{N} \\epsilon_i \\tag{28}\n",
    "\\end{align}\n",
    "\n",
    "where the energy matrix is predicated on an assumption of a linear binding model in which each base within the binding site region contributes a specific value ($\\epsilon_i$ for the $i^{th}$ base in the sequence) to the total binding energy.\n",
    "\n",
    "Energy matrices are either given in A.U. (arbitrary units), or if the gene has a simple repression or activation architecture with a single RNA polymerase (RNAP) site, are assigned $k_B T$ energy units following the procedure developed by [Kinney _et al_.](https://www.pnas.org/content/107/20/9158.long) and validated on the _lac_ operon from [Stephanie Barnes _et al._ paper in _PLoS Comp. Biol._](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006226).\n",
    "\n",
    "\\begin{align}\n",
    "E = \\sum^T_{i = A} \\sum^L_{j = 1} \\theta_{ij} \\delta_{ij} \\tag{29}\n",
    "\\end{align}\n",
    "\n",
    "is a non-linear model for creating energy matrices. However, what we want to find is the set of model parameters, $\\theta_{ij}$, that maximizes the probability\n",
    "\n",
    "\\begin{align}\n",
    "p(\\{ \\mu_s | \\theta \\}) \\tag{30}\n",
    "\\end{align}\n",
    "\n",
    "where $\\{ \\mu_s \\}$ is the set of Reg-Seq data for each sequence $s$.\n",
    "\n",
    "The Reg-Seq measurements are independent, so\n",
    "\n",
    "\\begin{align}\n",
    "p(\\{ \\mu_s | \\theta \\}) = \\prod^N_{s=1} p(\\mu_s | \\theta) \\tag{31}\n",
    "\\end{align}\n",
    "\n",
    "Additionally, the model parameters $\\theta$ only affects the probability in **(Eqn. 29)**, though the model predictions, in this case are given in **(Eqn. 27)** by the energy prediction $E_s$.\n",
    "\n",
    "\\begin{align}\n",
    "p(\\{ \\mu_s \\} | \\theta ) = p(\\{ \\mu_s \\} | E_s ) \\tag{32}\n",
    "\\end{align}\n",
    "\n",
    "While we could calculate this probability by assuming an error model, [Kinney _et al._ (2008)](https://pdfs.semanticscholar.org/56ce/a3cb3609844a0df0554f99524dbb96479c2d.pdf) showed that averaging over error models leads to an expression for likelihood in terms of mutual information,\n",
    "\n",
    "\\begin{align}\n",
    "p(\\{ \\mu_s \\} | \\theta ) \\propto 2^{N I (\\mu, E)} \\tag{33}\n",
    "\\end{align}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
